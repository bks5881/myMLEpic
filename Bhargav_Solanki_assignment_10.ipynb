{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You: Bhargav Solanki\n",
    "\n",
    "Teammate: Carmen Sangro Prieto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tenth lab. Neural networks are more a class of tools than a single tool, though the foundation you built last week should enable you to understand what is going on here without too much trouble.\n",
    "\n",
    "There is relatively little coding this week, which is unfortunate: we are starting to touch topics that require more than a lab's worth of practice to achieve basic proficiency. Rather than overloading you of work, this week we focus a bit more on foundations and give you time to study; then we should hit more interesting and fun applications over the next lectures with Deep Learning and Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 14 points (out of 21 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Explain in English what is the distinctive feature of a residual network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to feed forward neural networks, which is directed acyclic  fully connected graph. A residual NN is when graph is not fully connected and actually some connections across layers are skipped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[2pt]** Write the full equation of a network with structure [2, 4, 1] (same as last week), but this time add (i) biases on all neurons, and (ii) self-recurrent connections only on the hidden layer. How many weights does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I would suggest starting from your answer from last week, fixing it based on the solution if you need to and have not already, then add what you need.\n",
    "- To avoid changing the indices of the weights, you can simply call bias weights $b$ rather than $w$, and recurrent connections $r$.\n",
    "- The main thing to remember is: each line has the weights entering one destination neuron, and each column refers to one of the inputs to the layer.\n",
    "- Then for a recurrent network, remember to pass the output of all neurons of the same layer (technically representing the previous-step activations, initialized as `0`s) as inputs to each neuron.\n",
    "\n",
    "The network has three layers:\n",
    "- An input layer (no neurons!) with two elements $(x_1, x_2)$\n",
    "- One hidden layer composed of four neurons $(n_1, n_2, n_3, n_4)$\n",
    "- The output layer with only one neuron $(n_5)$\n",
    "\n",
    "We will need to add biases and recurrencies this time: it could be helpful to describe the inputs/outputs for each layer together with the weight matrix. \n",
    "- $X$ is the network input, same as before\n",
    "- Then come the recurrent connections: all the outputs of the neurons of the hidden layer, technically from the previous time step (initialize as zeros)\n",
    "- Finally the bias input, the constant 1 that will be multiplied by the bias weight\n",
    "- $X_{hid}$ is the actual full input to the hidden (and first) layer: all three above\n",
    "- $W_h$ is the weight matrix for ALL the connections entering the hidden layer in the columns, while the rows group the connections entering each neuron\n",
    "- The output can be written with $n_i$ same as we did last time; the don't forget you will need the bias also for the output layer (but no recursion!)\n",
    "- You can call $X_{out}$ the input to the output (and second) layer, and $W_{out}$ its weight matrix.\n",
    "And do not underestimate the value of a quick sketch on a piece of paper! Or head to [draw.io](https://draw.io) if you want a computer drawing that is easy, quick and professional looking.\n",
    "\n",
    "Remember that the output can be interpreted as one scalar, but is in principle a vector with one element (because having only one output is a special case, normally you need a list of outputs here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### The inputs are :\n",
    "\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "##### The number of neurons is :\n",
    "$$\n",
    "n_{hid} = \\begin{pmatrix}\n",
    "n_1\\\\\n",
    "n_2\\\\\n",
    "n_3\\\\\n",
    "n_4\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "##### The Biases are:\n",
    "$$\n",
    "b = \\begin{pmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "b_3\\\\\n",
    "b_4\\\\\n",
    "b_5\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "##### The Weights for hidden neuron's output that go as input are:\n",
    "$$\n",
    "r = \\begin{pmatrix}\n",
    "r_{11}\\\\\n",
    "r_{12}\\\\\n",
    "r_{13}\\\\\n",
    "r_{14}\\\\\n",
    "r_{21}\\\\\n",
    "r_{22}\\\\\n",
    "r_{23}\\\\\n",
    "r_{24}\\\\\n",
    "r_{31}\\\\\n",
    "r_{32}\\\\\n",
    "r_{33}\\\\\n",
    "r_{34}\\\\\n",
    "r_{41}\\\\\n",
    "r_{42}\\\\\n",
    "r_{43}\\\\\n",
    "r_{44}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "##### The outputs of hidden layers at time (t-1):\n",
    "$$\n",
    "n = \\begin{pmatrix}\n",
    "n_1^{t-1}\\\\\n",
    "n_2^{t-1}\\\\\n",
    "n_3^{t-1}\\\\\n",
    "n_4^{t-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "##### We have layer 1 weights as :\n",
    "$$\n",
    "W_{in}\n",
    "= \\begin{bmatrix}\n",
    "w_1 & w2\\\\ \n",
    "w_3 & w_4\\\\\n",
    "w_5 & w6\\\\\n",
    "w_7 & w8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### We have hidden weights as :\n",
    "$$\n",
    "W_{hid}\n",
    "= \\begin{bmatrix}\n",
    "w_9 & w_{10} & w_{11} & w_{12}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### The output neuron is :\n",
    "$$\n",
    "n_{out} = \\begin{pmatrix}\n",
    "n_5\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "##### The act equation for [2,4,1] is :\n",
    "$$\n",
    "= \\sigma [\\\\\n",
    "w_9 \\sigma(w_1.x_1^{t} + w_2.x_2^{t} + r_{11}.n_1^{t-1}+ r_{12}.n_2^{t-1}+ r_{13}.n_3^{t-1}+ r_{14}.n_4^{t-1} + 1.b_1 )\\\\\n",
    "+ \\\\\n",
    "w_{10} \\sigma(w_3.x_1^{t} + w_4.x_2^{t} + r_{21}.n_1^{t-1}+ r_{22}.n_2^{t-1}+ r_{23}.n_3^{t-1}+ r_{24}.n_4^{t-1} + 1.b_2)\\\\\n",
    "+ \\\\\n",
    "w_{11} \\sigma(w_5.x_1^{t}+ w_6.x_2^{t} + r_{31}.n_1^{t-1}+ r_{32}.n_2^{t-1}+ r_{33}.n_3^{t-1}+ r_{34}.n_4^{t-1} + 1.b_3)\\\\\n",
    "+\\\\\n",
    "w_{12} \\sigma(w_7.x_1^{t} + w_8.x_2^{t}+ r_{41}.n_1^{t-1}+ r_{42}.n_2^{t-1}+ r_{43}.n_3^{t-1}+ r_{44}.n_4^{t-1} + 1.b_4)\\\\\n",
    "+\\\\\n",
    "1.b_5\n",
    "]\n",
    "$$\n",
    "\n",
    "##### The total number of weights are 8 weights for each hidden neuron, i.e 2 from inputs, 1 weight of bias and 4 for recurrence times 4 ( number of neurons) so 28 for the hidden layer. Plus 4 weights to output nueron from the hidden layer input plus a weight bias. So 28 + 4 + 1 = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[2pt]** A neural network has only one layer of two convolutional neurons with identity activation. Below you will find respective kernels $W_1$ and $W_2$ and input $X$. Activate the network on the input by hand showing all calculation. Assume no padding and state explicitly the expected output size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to understand what you need to explain about your calculations if you actually start doing them :) just mark what you actually input in the calculator, what the calculator returns, and what calculation you are confident skipping.\n",
    "\n",
    "$$\n",
    "W_1 = \n",
    "\\begin{pmatrix}\n",
    "-1 & 1 & -1 \\\\\n",
    "-1 & 1 & -1 \\\\\n",
    "-1 & 1 & -1\n",
    "\\end{pmatrix}\n",
    "\\quad,\\quad\n",
    "W_2 = \n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    " 1 &  1 &  1 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "2 & 3 & 1 & 3 & 2 \\\\\n",
    "2 & 3 & 1 & 3 & 2 \\\\\n",
    "2 & 3 & 1 & 3 & 2 \\\\\n",
    "2 & 3 & 1 & 3 & 2 \\\\\n",
    "2 & 3 & 1 & 3 & 2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask starting  0 0\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "Total is = 0\n",
      "Mask starting  0 1\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "Total is = -15\n",
      "Mask starting  0 2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "Total is = 0\n",
      "Mask starting  1 0\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "Total is = 0\n",
      "Mask starting  1 1\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "Total is = -15\n",
      "Mask starting  1 2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "Total is = 0\n",
      "Mask starting  2 0\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "-1x2 =  -2\n",
      "1x3 =  3\n",
      "-1x1 =  -1\n",
      "Total is = 0\n",
      "Mask starting  2 1\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "-1x3 =  -3\n",
      "1x1 =  1\n",
      "-1x3 =  -3\n",
      "Total is = -15\n",
      "Mask starting  2 2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "-1x1 =  -1\n",
      "1x3 =  3\n",
      "-1x2 =  -2\n",
      "Total is = 0\n",
      "Output for W1 =  [[0, -15, 0], [0, -15, 0], [0, -15, 0]]\n",
      "Mask starting  0 0\n",
      "-1x2 =  -2\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "1x2 =  2\n",
      "1x3 =  3\n",
      "1x1 =  1\n",
      "-1x2 =  -2\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "Total is = -6\n",
      "Mask starting  0 1\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "1x3 =  3\n",
      "1x1 =  1\n",
      "1x3 =  3\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "Total is = -7\n",
      "Mask starting  0 2\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "-1x2 =  -2\n",
      "1x1 =  1\n",
      "1x3 =  3\n",
      "1x2 =  2\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "-1x2 =  -2\n",
      "Total is = -6\n",
      "Mask starting  1 0\n",
      "-1x2 =  -2\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "1x2 =  2\n",
      "1x3 =  3\n",
      "1x1 =  1\n",
      "-1x2 =  -2\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "Total is = -6\n",
      "Mask starting  1 1\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "1x3 =  3\n",
      "1x1 =  1\n",
      "1x3 =  3\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "Total is = -7\n",
      "Mask starting  1 2\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "-1x2 =  -2\n",
      "1x1 =  1\n",
      "1x3 =  3\n",
      "1x2 =  2\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "-1x2 =  -2\n",
      "Total is = -6\n",
      "Mask starting  2 0\n",
      "-1x2 =  -2\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "1x2 =  2\n",
      "1x3 =  3\n",
      "1x1 =  1\n",
      "-1x2 =  -2\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "Total is = -6\n",
      "Mask starting  2 1\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "1x3 =  3\n",
      "1x1 =  1\n",
      "1x3 =  3\n",
      "-1x3 =  -3\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "Total is = -7\n",
      "Mask starting  2 2\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "-1x2 =  -2\n",
      "1x1 =  1\n",
      "1x3 =  3\n",
      "1x2 =  2\n",
      "-1x1 =  -1\n",
      "-1x3 =  -3\n",
      "-1x2 =  -2\n",
      "Total is = -6\n",
      "Output for W2 =  [[-6, -7, -6], [-6, -7, -6], [-6, -7, -6]]\n"
     ]
    }
   ],
   "source": [
    "# It was too much work to do the calculations. I wrote a script to show the calculations.\n",
    "# If you run this code, it will show the output printed.Step by step.\n",
    "def helper(w,x):\n",
    "    res = []\n",
    "    for row_x in range(0, len(x)- len(w)+1):\n",
    "        masks = []\n",
    "        for col_x in range(0, len(x[row_x])- len(w[0])+1):\n",
    "            total_mask = 0\n",
    "            print(\"Mask starting \", row_x, col_x)\n",
    "            for i in range(0, len(w)):\n",
    "                for j in range(0, len(w[0])):\n",
    "                    val = w[i][j]* x[row_x+i][col_x+j]\n",
    "                    print(\"\"+str(w[i][j])+\"x\"+ str(x[row_x+i][col_x+j]) + \" = \" ,val )\n",
    "                    total_mask += val\n",
    "            print(\"Total is =\", total_mask )\n",
    "            masks.append(total_mask)\n",
    "        res.append(masks)\n",
    "    return res\n",
    "            \n",
    "w1 = [[-1, 1, -1],[-1, 1, -1], [-1, 1, -1]]\n",
    "w2 = [[-1, -1, -1], [1,1,1], [-1, -1,-1]]\n",
    "x = [[2,3,1,3,2],[2,3,1,3,2],[2,3,1,3,2],[2,3,1,3,2],[2,3,1,3,2]]\n",
    "kernal1 = helper(w1,x)\n",
    "print(\"Output for W1 = \", kernal1)\n",
    "kernal2 = helper(w2, x)\n",
    "print(\"Output for W2 = \", kernal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 **[2pt]** Look at the activations of the two neurons from 1.3 and discuss why they are so different. Explain in particular the regularities both in the inputs and in the kernels. Then go one step further and explain, to the best of your understanding, which types of features are detected by the two kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is another open question: as long as you do not write anything wrong, while showing competence and intuition, you will get the points.\n",
    "- Hint: focus on thinking about the _patterns_ that you can see by eye both in the data matrix and in the kernels. Try to go for an intuitive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 is ttrying to extract features from columns by assigning centre column poisitve weights (1,1,1) in the centre column. And w2 focuses on extracting features from the rows, specifically the centre row of the wieghts in the kernal is giving positive weights to the rows. They are both similar in the sense that feature to be focused is either column or rows. So, as per my understanding, w1 will pick up vertical features and w2 will pick up horizontal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 **[1pt]** Activate a $3x3$ max pooling layer on the outputs of the two convolutions from your answer to question 1.3. Assume again no padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying max pooling on output of w1 with x\n",
    "kernal1 =  [[0, -15, 0], [0, -15, 0], [0, -15, 0]]\n",
    "this is a 3 x 3 matrix and so is our max pooling. Hence, the result is max(0, -15, 0, 0, -15, 0,0, -15, 0) = 0\n",
    "Applying max pooling on output of w2 with x\n",
    "kernal1 =  [[-6, -7, -6], [-6, -7, -6], [-6, -7, -6]]\n",
    "this is a 3 x 3 matrix and so is our max pooling. Hence, the result is max(-6, -7, -6, -6, -7, -6,-6, -7, -6) = -6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 **[1pt]** Explain in one sentence what is an autoencoder. Why do autoencoders have an hourglass shape? Could you design an autoencoder with a different shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder takes in a set of inputs and reduces the deimesnionality of the input. So for example, if you give 5 inputs, it will apply a neural network and give an output in the output layer with a vector length of 2. So dimensionality reduced from 5 to 2 in this example. Now , decoder will take in these two inputs and try to produce the original output. In our example, the output of our encoder of vector length 2 is given as an input to the decoder, which is again a NN, and then results in a output of lenght 5, trying to get the same values as input. It looks like a hour glass because, \n",
    "High dimensions as input , reduced dimensions in the middle, and then same number of high deimensions in the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[4pt]** Below is last week's implementation of a neural network augmented into a fully-connected RNN with bias connections. Fix it by writing the missing code as marked by `?`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unless otherwise stated, a RNN has fully-connected self-recurrent connections on each layer.\n",
    "- You should know exactly which connections to add if you answered the RNN question in the fundamentals.\n",
    "- For the bias: remember that you need all elements in `state` to be longer by one element: put actual `1`s in these last positions at initialization, then never touch them again.\n",
    "- Recurrencies: you need to make space in the input to each layer for its own output. I typically sort them as [input, recurrencies, bias], but order is not important: consistency is. Make sure all your sizes are correct.\n",
    "- When calculating the size of an input now you need to use `struct` twice: once for the size of the layer entering (something like `struct[nlay]`) and once for the size of the output that goes back as input in the recursion (hence `struct[nlay+1]`). HINT: to make the pairs for each layer execute and understand the following: `zip(struct, struct[1:])`\n",
    "- When you activate a layer remember to copy the activation to both (i) its output and (ii) its input, at the correct indices.\n",
    "- It's easier to compute the size of each input beforehand, then use it to make the `state` list. Then for each weight matrix you can take the number of rows from the structure (as before), and the number of inputs from the `state` sizes.  Don't worry about duplicating the activation in the layer's input, it's actually faster because you have a ready numpy array rather than composing at activation.\n",
    "- Remember to initialize the recurrent output to `0`. Simplest way is to initialize `state` using `np.zeros()` instead of `np.empty()`. Then set the last value of each `state` element to `1` for the bias.\n",
    "- I used myself `import IPython; IPython.embed()` heavily to get this to work. You can also call (once!) `%pdb` to drop in the debugger on error. Keep calm and check the dimensions.\n",
    "- The layer activation function should change because you are saving the activation to two locations (and at specific indices, not direct substitution like before), but the network activation function should change just marginally (add indices to insert input in state)\n",
    "- To _convolve_ with stride 1 and no padding a window of size two on a 1D list (take a pair at a time, advance by one) in Python you can use `zip(lst, lst[1:])`.\n",
    "- This question only refers to neural networks, not learning algorithm, so leave backpropagation out and do not worry about unrolling the network.\n",
    "- Again activate it on a simple input to verify everything is works. The input should be exactly the same as last week's (as the network architecture).\n",
    "- Think: how many weights do you expect to have? Remember that you have 3 matrices (for the 3 layers of neurons), in each the number of rows is unchanged (because you have one per each neuron) but the inputs now are not only connections from the previous layer, you also have recursion (take the output of this layer as its own input) and bias (constant 1 appended to the inputs).\n",
    "<!-- Secret hint: my tally is (4+3+1)*3 + (3+4+1)*4 + (4+3+1)*3 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, struct):\n",
    "        # These are basic, copy+paste from FFNN\n",
    "        self.struct = struct\n",
    "        self.nlayers = len(self.struct)\n",
    "        self.nins, *self.hidden, self.nouts = self.struct\n",
    "        self.sigma = lambda x: ( 1 / (1 + np.exp(-x)))\n",
    "        # Each `state` is an input for next layer: it now includes rec and bias\n",
    "        state_sizes = [inp+1+rec for inp, rec in zip(self.struct, self.struct[1:])]\n",
    "        # Notice the `zip` above ends when the second list reaches the end (1 shorter)\n",
    "        # Last `state` is only the output of the last layer (no rec/b)\n",
    "        state_sizes += [self.nouts]\n",
    "        # We can now build the state of the network\n",
    "        temp_state = []\n",
    "        for size in state_sizes:\n",
    "            temp_state.append([0]*size)\n",
    "        self.state = np.array(temp_state)\n",
    "        #print(self.state)\n",
    "        # We will need to access inputs and recurrencies by index: the following helps\n",
    "        self.inp_idxs = [range(0, val) for val in self.struct]\n",
    "        #self.inp_idxs = np.array([range(0, val) for val in self.struct])\n",
    "        self.rec_idxs = [range(s, s+end) for s, end in zip(self.struct, self.struct[1:])]\n",
    "        #print(self.rec_idxs)\n",
    "        # Finally, fix the bias input in the last position of all input `state`s\n",
    "        # Just set and forget, and no need for indices because we won't access it again\n",
    "        for s in self.state: s[-1] = 1\n",
    "        #print(self.struct[1:], self.struct)    \n",
    "        # The `state` sizes now correspond to the row lengths (ncols) for the weight matrices\n",
    "        self.wsizes = [[row, col+row+1] for row, col in zip(self.struct[1:], self.struct)]\n",
    "        # Finally: weight initialization. Bad practice to hardcode this, but ok here\n",
    "        self.weights = [np.random.normal(size=ws) for ws in self.wsizes]\n",
    "        #print(self.weights)\n",
    "\n",
    "    # The layer activation is unchanged: sigma(W.dot(X)) -- only W and X (=state) differ\n",
    "    def act_layer(self, nlay):\n",
    "        #print(\"For layer =\",nlay,\"val is \" , self.weights[nlay].dot(self.state [nlay]))\n",
    "        return self.sigma(self.weights[nlay].dot(self.state [nlay]))\n",
    "\n",
    "    # The network activation only writes the act twice this time: output & rec-input\n",
    "    def act_net(self, inp):\n",
    "        assert len(inp) == self.nins, f\"got input `{inp}`, expected np.array of length `{self.nins}`\"\n",
    "        #print(self.state[0], inp)\n",
    "        zeroth = [val for val in self.inp_idxs[0]]\n",
    "        for i in range(0, len(zeroth)):\n",
    "            val = zeroth[i]\n",
    "            self.state[0][val] = inp[i]\n",
    "        #self.state[0][self.inp_idxs[0]] = inp\n",
    "        for nlay in range(self.nlayers-1):\n",
    "            act = self.act_layer(nlay)\n",
    "            #print(\"Output = \", act)\n",
    "            # This time the layer activation goes in two places:\n",
    "            # - In the input indices of the output of this layer / input to next\n",
    "            temp = []\n",
    "            for val in self.inp_idxs[nlay+1]:\n",
    "                temp.append(val)\n",
    "            #    self.state[nlay][val] = act[]\n",
    "            # - In the recurrent indices of the input to this layer\n",
    "            #for val in self.rec_idxs[nlay]:\n",
    "            temp1 = [val for val in self.inp_idxs[nlay+1]]\n",
    "            for i in range(0, len(temp1)):\n",
    "                val = temp1[i]\n",
    "                self.state[nlay+1][val] = act[i]\n",
    "            #self.state[nlay][self.inp_idxs[nlay+1]] = act\n",
    "            temp1 = [val for val in self.rec_idxs[nlay]]\n",
    "            for i in range(0, len(temp1)):\n",
    "                val = temp1[i]\n",
    "                self.state[nlay][val] = act[i]\n",
    "            #self.state[nlay+1][self.rec_idxs[nlay]] = act\n",
    "        return self.state[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation 1: [0.2615155648876883, 0.9132859193763553, 0.19136407060168362]\n",
      "activation 2: [0.2819070270928628, 0.9113427021661625, 0.13965234932150672]\n",
      "activation 3: [0.31739349703632463, 0.8951336001767031, 0.18966439891906361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/w2l564n124q9v61h6kxcz3tm0000gn/T/ipykernel_9772/2854630062.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.state = np.array(temp_state)\n"
     ]
    }
   ],
   "source": [
    "struct = [4,5,4,3]\n",
    "#struct = [2, 4, 1]\n",
    "inputs = [3,2,4,3]\n",
    "#inputs = [2,4]\n",
    "net = RecurrentNeuralNetwork(struct)\n",
    "# We expect the activation to change this time upon multiple calls on the same input\n",
    "# This is because the `state` of the RNN is maintained in the recurrent connections\n",
    "print(\"activation 1:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 2:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 3:\", net.act_net(np.array(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[2pt]** Write a Python function for 2D convolution, then run it on a randomly generated input matrix and show the output.\n",
    "\n",
    "- You need to write a function that takes a 2D input and a function to convolve as parameters, then convolves the function over the inputs to produce the output.\n",
    "- The window size is not specified: you can use a 3x3 to keep it simple since you saw that in the examples.\n",
    "- The function to convolve is not specified: no need for it to be a neural network, something as simple as `sum` or a quick lambda calling the numpy `x.sum()` would work perfectly well. Remember that the important part is that it should take a high-dimensional (3x3?) input and output only one value.\n",
    "- The size of the input matrix is not specified: with a 3x3 mask we could go as small as 5x5 with no padding and that would still show that the convolution works.\n",
    "- Then remember: the neural networks are just other functions than `sum`, but behave exactly the same way. Convolving a neural network only allows you to learn the function rather than hardcoding it, but the convolution process is independent.\n",
    "- Answer the question until the end: show that you know how to create a matrix of random numbers, and of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 16],\n",
       "       [24, 28]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function to create the slices\n",
    "def create_slices(matrix, slice_x, slice_y):\n",
    "    width = len(matrix[0])\n",
    "    height = len(matrix)\n",
    "    slices = []\n",
    "    for i in range(0, height - slice_y + 1):\n",
    "        for j in range(0, width - slice_x + 1):\n",
    "            slices.append(\n",
    "                [\n",
    "                    [matrix[a][b] for b in range(j, j + slice_x)]\n",
    "                    for a in range(i, i + slice_y)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "    return slices\n",
    "\n",
    "\n",
    "def convolution(x, k):\n",
    "    \n",
    "    #create final matrix\n",
    "    k_shape = k.shape\n",
    "    final_matrix = np.zeros((k.shape), dtype=\"int\")\n",
    "    \n",
    "    #sizes of the slices\n",
    "    slice_x = k.shape[0]\n",
    "    slice_y = k.shape[0]\n",
    "    \n",
    "    #call slices function\n",
    "    slices = create_slices(x, slice_x, slice_y)\n",
    "    \n",
    "    #iterate over every slice\n",
    "    list_slices = []\n",
    "    for sl in slices:\n",
    "        list_slices.append((sl*k).sum())\n",
    "   \n",
    "    calculated = []\n",
    "    while list_slices != []:\n",
    "        calculated.append(list_slices[:len(final_matrix)])\n",
    "        list_slices = list_slices[len(final_matrix):]\n",
    "\n",
    "    for i in range(len(final_matrix)):\n",
    "        for j in range(len(final_matrix)):\n",
    "            final_matrix[i][j] = calculated[i][j]\n",
    "            \n",
    "    return final_matrix\n",
    "prueba = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "two = np.full((2,2),1)\n",
    "convolution(prueba, two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask starting  0 0\n",
      "Mask starting  0 1\n",
      "Mask starting  0 2\n",
      "Mask starting  1 0\n",
      "Mask starting  1 1\n",
      "Mask starting  1 2\n",
      "Mask starting  2 0\n",
      "Mask starting  2 1\n",
      "Mask starting  2 2\n",
      "convolution is  [[-6, -7, -6], [-6, -7, -6], [-6, -7, -6]]\n",
      "Mask starting  0 0\n",
      "Mask starting  0 1\n",
      "Mask starting  0 2\n",
      "Mask starting  1 0\n",
      "Mask starting  1 1\n",
      "Mask starting  1 2\n",
      "Mask starting  2 0\n",
      "Mask starting  2 1\n",
      "Mask starting  2 2\n",
      "convolution is  [[2.6474025054452777, 3.0679413892412755, 3.4980944193745924], [3.841590679065327, 3.433079687685648, 3.5546115117613963], [3.503468758614041, 2.908808701859149, 2.824469740208538]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def convolution_helper(w,x, convolve):\n",
    "    res = []\n",
    "    for row_x in range(0, len(x)- len(w)+1):\n",
    "        masks = []\n",
    "        for col_x in range(0, len(x[row_x])- len(w[0])+1):\n",
    "            total_mask = []\n",
    "            print(\"Mask starting \", row_x, col_x)\n",
    "            for i in range(0, len(w)):\n",
    "                for j in range(0, len(w[0])):\n",
    "                    val = w[i][j]* x[row_x+i][col_x+j]\n",
    "                    #print(\"\"+str(w[i][j])+\"x\"+ str(x[row_x+i][col_x+j]) + \" = \" ,val )\n",
    "                    total_mask.append( val)\n",
    "                \n",
    "            #print(\"Total is =\", total_mask )\n",
    "            # applying convolution\n",
    "            temp = convolve(total_mask)\n",
    "            masks.append(temp)\n",
    "            \n",
    "        res.append(masks)\n",
    "    return res\n",
    "w1 = [[-1, -1, -1], [1,1,1], [-1, -1,-1]]\n",
    "x = [[2,3,1,3,2],[2,3,1,3,2],[2,3,1,3,2],[2,3,1,3,2],[2,3,1,3,2]]\n",
    "# here np.sum is my convolution\n",
    "kernal1 = convolution_helper(w1,x, np.sum)\n",
    "print(\"convolution is \", kernal1)\n",
    "random_x = np.random.rand(5,5)\n",
    "random_weights = np.random.rand(3,3)\n",
    "kernal2 = convolution_helper(random_weights,random_x, np.sum)\n",
    "print(\"convolution is \", kernal2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handwritten digit recognition with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning, we need to cross a gap in exercise complexity. On one hand you are ready to understand the inner work of a DL library like Keras, on the other asking you for such a task on top of today's lecture is too much even for this course :) So let's leave the creative part for next weeks, where we will see some more advanced applications anyway, and focus today on what we learned and on a new skill: how to justify your code.\n",
    "\n",
    "Below is a tutorial from the Keras website on convolutional networks [[source]](https://keras.io/examples/vision/mnist_convnet/). It uses the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a standard dataset for handwritten character recognition. Keras offers you a backend to automatically download the dataset, similarly to what we did so far with Seaborn and Iris.\n",
    "\n",
    "The code should work as is (did you `pipenv install tensorflow keras`?), but take a while to run. Read it line by line, really study and understand it, feel free to change it so that it runs in few seconds if you wish to play with it; then answer the questions below.\n",
    "\n",
    "NOTE: you don't need to run the code to answer any of the questions below. If you used PyTorch at the last lecture, this is your chance to try out Keras. If you cannot (looking at you M1 users), you can use Colab for this assignment, or replace the Keras tutorial with a PyTorch equivalent [such as this](https://pythonguides.com/pytorch-mnist/). This only affects 4.3, while 4.1 and 4.2 should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.3547 - accuracy: 0.8910 - val_loss: 0.0781 - val_accuracy: 0.9790\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.1069 - accuracy: 0.9676 - val_loss: 0.0564 - val_accuracy: 0.9857\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 16s 37ms/step - loss: 0.0828 - accuracy: 0.9743 - val_loss: 0.0463 - val_accuracy: 0.9888\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0696 - accuracy: 0.9782 - val_loss: 0.0422 - val_accuracy: 0.9892\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.0440 - val_accuracy: 0.9882\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 16s 37ms/step - loss: 0.0548 - accuracy: 0.9826 - val_loss: 0.0414 - val_accuracy: 0.9883\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0503 - accuracy: 0.9841 - val_loss: 0.0375 - val_accuracy: 0.9897\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0485 - accuracy: 0.9849 - val_loss: 0.0371 - val_accuracy: 0.9898\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.0310 - val_accuracy: 0.9915\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0418 - accuracy: 0.9868 - val_loss: 0.0324 - val_accuracy: 0.9908\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 0.0330 - val_accuracy: 0.9910\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 16s 37ms/step - loss: 0.0388 - accuracy: 0.9876 - val_loss: 0.0306 - val_accuracy: 0.9912\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 0.0374 - accuracy: 0.9878 - val_loss: 0.0290 - val_accuracy: 0.9918\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0344 - accuracy: 0.9886 - val_loss: 0.0286 - val_accuracy: 0.9925\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.0339 - accuracy: 0.9888 - val_loss: 0.0280 - val_accuracy: 0.9922\n",
      "Test loss: 0.025225576013326645\n",
      "Test accuracy: 0.991599977016449\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Simple MNIST convnet\n",
    "Author: [fchollet](https://twitter.com/fchollet)\n",
    "Date created: 2015/06/19\n",
    "Last modified: 2020/04/21\n",
    "Description: A simple convnet that achieves ~99% test accuracy on MNIST.\n",
    "SOURCE: https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "print(\"Hello\")\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the data\n",
    "\"\"\"\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Train the model\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "## Evaluate the trained model\n",
    "\"\"\"\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 **[1pt]** Data plotting: plot the first 9 images in MNIST using a 6x6 subplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see what the MNIST looks like. I showed how to use subplots in a recent solution -- do you remember where it was? You'll need the ability to search quickly for what you need to complete the exam in time. Try timing how long it takes you to answer this question (no seriously challenge your teammate on who solves this the quickest and feel free to brag about it in your solution below).\n",
    "- Add the label on the title to see how the numbers are represented: do you see the connection to last week's `species`?\n",
    "- After you obtain the axis from `plt.subplots()`, you can print a 2D image using `ax[?,?].imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data again\n",
    "(x_train2, y_train2), (x_test2, y_test2) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# select first 9 images\n",
    "images = x_train2[:9]\n",
    "labels = y_train2[:9]\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(3,3)\n",
    "for i in range(9):\n",
    "    ax = axes[i//3, i%3]\n",
    "    ax.imshow(images[i], cmap='gray_r')\n",
    "    ax.set_title('Label: {}'.format(labels[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 **[3pt]** Explain the following lines in the Keras MNIST Tutorial code (in English): 27, 48, 51, 52, 67, 68\n",
    "\n",
    "- To answer this question you need to show complete competence, as if you wrote this code yourself and you were asked to explain your choices at an oral exam.\n",
    "- For each of the lines mentioned, check the code provided and explain it thoroughly\n",
    "- For each variable, explain its meaning, its use, and the choice of value assigned\n",
    "- For each function call, explain what it does, the meaning of all parameters, and the choices of all values.\n",
    "- Reading the code like \"assign 12 to variable `epochs`\" will not constitute an acceptable answer.\n",
    "- Reading the code like \"creates a new Sequential\" is also not acceptable: check the documentation for `Sequential`, understand what the call does, and present your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Line 27: we are **loading the MNIST dataset** from the Keras library and assigning its different components to different variables i.e. since there exists a training and a test dataset, we are differetiating both, and also selecting the x and y values respectively.\n",
    "\n",
    "2) Line 48: we are creating our **model**, by calling the Keras class \"sequential\", which basically groups a linear stack of layers, and we pass a series of arguments.\n",
    "\n",
    "3) Line 51: we are performing **2D convolution**, where 32 is the dimensionality of the output space; we are using a kernel of size 3x3 and we're also using the rectified linear unit activation function.\n",
    "\n",
    "4) Line 52: we're conveying a **max-pooling** that returns an output of size 2x2. Basically, the max-pooling neuron outputs the maximum of its inputs over its receptive field.\n",
    "\n",
    "5) Line 67: we're establishing a **batch size** of 64 (originally 128). The batch size is a hyperparameter that defines the number of samples that are going to be propagated through the network.\n",
    "\n",
    "6) Line 68: we're establishing 6 (originally 15) **epochs**. The epochs are a hyperparameter that states the number of times that the algorithm will work through the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 **[2pt]** Tweak the variables in the Keras MNIST code to be able to run it in a decent time. Performance plotting: plot the model's accuracy and loss over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is almost for free since you did the same visualization last week, but you need to get the code to run first.\n",
    "- Also you may want to make sure your changes include setting 'accuracy' and the `history` variable, or at the end of the run you could end up with still nothing to show :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdc0f325f10>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfBklEQVR4nO3de5gU9Z3v8fe3u2cY7tcR5aKga4xIAHUAN54QzcXgJWLi8XiLAhqIz6Ou7p41GhPXZ9ecEzeebLI+uBKSReQYJUYxMcZoxFzQEzAMNxHxQlBhgI3DPXKb6e7v+aNqZpqhZ6aBnqnpms/reYqu+tWvqr/dD/Op6uqqanN3RESk9CWiLkBERIpDgS4iEhMKdBGRmFCgi4jEhAJdRCQmUlE98aBBg3zEiBFRPb2ISElavnz5NnevzDcvskAfMWIE1dXVUT29iEhJMrMPWpqnQy4iIjGhQBcRiQkFuohITLR5DN3M5gKXAB+6++g88w34d+AiYB8wzd1XFLtQESkt9fX11NTUcODAgahLKUkVFRUMGzaMsrKygpcp5EvRecAsYH4L8y8ETg2HicDD4aOIdGE1NTX07t2bESNGEOz3SaHcne3bt1NTU8PIkSMLXq7NQy7uvhjY0UqXKcB8DywF+pnZCQVXICKxdODAAQYOHKgwPwpmxsCBA4/4000xjqEPBTblTNeEbYcxs5lmVm1m1bW1tUV4ahHpzBTmR+9o3rtinIee71nz3pPX3ecAcwCqqqp0317p8twdd8i644SPTjDgZD3ok3Ugb7/2+zNqeJ6sQzbbVGdjW8N4tqme3PnJdJa9B9NBGIR1tlStHzbSdm2HN/oh7R7+4zkt3jjj0Oc8rM9h7UfHW5jo2S1J74rCj40XqhiBXgMMz5keBmwpwnqlk8pknXQ2Sybr1Ge8cTodjtdnsmGbk8409U1nnWz4mMl6Y1vD8ln3xnVkPGzPBAFx+DJZMmGQZLNNAeM5gdcYhtlm057bvymEgvFgOpP1xseG8Yb6g9oIX0uWrHNIv4bxhtfQsExj8NH03HH2o0tPIFH7UdRldEqVdOu0gf4scIuZLSD4MnS3u28twnq7tPpMlgP1GQ7UNzxm2J8zvb8+Q30mS106GzxmvHG8Pp2lLhMM9WmnLpOhPh0E7cFwfn3O/Ny2YGgKznQ2Sybj1OeEcmcIomTCSJqRTBgJg4QZhI8N05YznrDgI2wi0TBtWO48cqYTkDQjET5HImGUJRNUlAXLBc9pJBOQSiTCfjT2TyWb92sab6otqMcgp5awLawnmA5f2yH9Dl2+PY5quDd/L5vem8Pf25zxRPg+m9HnwF84eVDPYIVhra2xw0ba6HfYjEOfw8J/LKfFGmccuh6zZn1a6NeSdDpNKpUK1xXdYaZCTlt8AjgPGGRmNcC9QBmAu88Gnic4ZXE9wWmL09ur2M7I3dlXl2HPgXp2769nz/40e/aH42HbvrowkOsyHEg3BXRDYO9vNn2gPkM6e+ypWZ5K0C2ZoCyVoDyZoCwVBFN5MkF5KtE43re8jPKkUZ5KkEokSCWNskSCZNJIJayxLZkwyhJGMpxOhUFVlkyQTIR9k4mc9qa+SWvq3xB4qUSiMewals83ngxrSDQEqOnYbClYt24bvdphL/RIXXbZZWzatIkDBw5w2223MXPmTF544QXuvvtuMpkMgwYN4uWXX+ajjz7i1ltvpbq6GjPj3nvv5fLLL6dXr1589FHwSeOpp57iueeeY968eUybNo0BAwawcuVKzjrrLK688kpuv/129u/fT/fu3XnkkUc47bTTyGQy3Hnnnbz44ouYGTNmzGDUqFHMmjWLZ555BoCXXnqJhx9+mIULFx7Ta20z0N396jbmO3DzMVXRCWSyzsYd+8JQbgrkPfvTh4TznobhQLpxuq3w7ZZK0L08SUUqSffyJN1SCSrKknQvSzKoV6pxXrewraIsET4G4xXhePO2bmEoNwR2Q2iXhwGr0JPO4p9/uZY3t+wp6jpHDenDvV88o81+c+fOZcCAAezfv5/x48czZcoUZsyYweLFixk5ciQ7dgQn8d1333307duXNWvWALBz58421/3OO++waNEikskke/bsYfHixaRSKRYtWsTdd9/N008/zZw5c3jvvfdYuXIlqVSKHTt20L9/f26++WZqa2uprKzkkUceYfr0Y98XjuzmXJ3Je9v28ndPrGTN5t1555cljb7dy+hTUUaf7mX07VHOiQN70qciFUyH8/p2L6NP99Qh070rUqSSuiBXJCoPPvhg457wpk2bmDNnDpMmTWo8v3vAgAEALFq0iAULFjQu179//zbXfcUVV5BMJgHYvXs3U6dO5d1338XMqK+vb1zvTTfd1HhIpuH5rrvuOh577DGmT5/OkiVLmD+/pUt9CtflA33hihru+fkbpJIJ/vnSMxjWv/thIV1RltDersgxKGRPuj38/ve/Z9GiRSxZsoQePXpw3nnnMXbsWN5+++3D+rp73r/z3Lbm54X37Nmzcfyee+7h/PPP55lnnuH999/nvPPOa3W906dP54tf/CIVFRVcccUVjYF/LLrsruNHB9P8w09X8Q9PruaMIX359W2fYuonR/DZ0wczfsQAPja4N8f3raB7eVJhLlKidu/eTf/+/enRowdvvfUWS5cu5eDBg/zhD3/gvffeA2g85HLBBRcwa9asxmUbDrkMHjyYdevWkc1mG/f0W3quoUODS3DmzZvX2H7BBRcwe/Zs0un0Ic83ZMgQhgwZwre//W2mTZtWlNfbJQN9Tc1uLnnwFX6+ajO3f+5UHp8xkSH9ukddlogU2eTJk0mn04wZM4Z77rmHc845h8rKSubMmcOXv/xlxo4dy5VXXgnAt771LXbu3Mno0aMZO3Ysv/vd7wC4//77ueSSS/jMZz7DCSe0fBH817/+db7xjW9w7rnnkslkGtu/+tWvcuKJJzJmzBjGjh3L448/3jjv2muvZfjw4YwaNaoor9fa88KE1lRVVXlH/8BFNuvM/X/v8a8vvMWgXt34wZXjmHjywA6tQaSrWLduHaeffnrUZXRqt9xyC2eeeSY33nhj3vn53kMzW+7uVfn6d5lj6Ns+Osg//mw1v3+7ls+PGsx3Lx9D/57lUZclIl3U2WefTc+ePfne975XtHV2iUB/9d1t/P2Tq9i9v577ppzBV845ScfFRSRSy5cvL/o6Yx3o9Zks//bSO8z+w585pbIX82+YwOkn9Im6LBGRdhHbQN+0Yx+3PrGSVZt2cfWE4dxzySh6lMf25YqIxDPQf7l6C3cvXAMGs645k0vGDIm6JBGRdherQN9Xl+afn32Tn1Zv4swT+/HgVWcyfECPqMsSEekQsQn0N7fs4dYnVrBh215uPv8Ubv/cxyjTJfciXVrujbW6gpIPdHdn/pIP+F/Pr6Nf9zIeu3Ei5/7NoKjLEhHpcCW9C7tzbx0z5i/n3mfXcu4pA/n1bZ9SmIvIYdydO+64g9GjR/OJT3yCn/70pwBs3bqVSZMmMW7cOEaPHs0rr7xCJpNh2rRpjX2///3vR1x94Up2D33phu3cvmAV2/ce5J5LRnHDufplcZFO69d3wX+tKe46j/8EXHh/QV0XLlzIqlWrWL16Ndu2bWP8+PFMmjSJxx9/nC984Qt885vfJJPJsG/fPlatWsXmzZt54403ANi1a1dx625HJRfo6UyWB3+7nlm/fZeTBvbkmannMnpo36jLEpFO7NVXX+Xqq68mmUwyePBgPv3pT7Ns2TLGjx/PDTfcQH19PZdddhnjxo3j5JNPZsOGDdx6661cfPHFXHDBBVGXX7CSC/SfLa/hwZff5fKzhvEvU86gZ7eSewkiXU+Be9LtpaV7Vk2aNInFixfzq1/9iuuuu4477riD66+/ntWrV/Piiy/y0EMP8eSTTzJ37twOrvjolFwaXnH2MI7vU8H5Hz8u6lJEpERMmjSJH/7wh0ydOpUdO3awePFiHnjgAT744AOGDh3KjBkz2Lt3LytWrOCiiy6ivLycyy+/nFNOOaVot7btCCUX6KlkQmEuIkfkS1/6EkuWLGHs2LGYGd/97nc5/vjjefTRR3nggQcoKyujV69ezJ8/n82bNzN9+nSy2SwA3/nOdyKuvnBd6va5ItJxdPvcY3ekt88t6dMWRUSkiQJdRCQmFOgi0m6iOqQbB0fz3inQRaRdVFRUsH37doX6UXB3tm/fTkVFxREtV3JnuYhIaRg2bBg1NTXU1tZGXUpJqqioYNiwYUe0jAJdRNpFWVkZI0eOjLqMLkWHXEREYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMVFQoJvZZDN728zWm9ldeeb3NbNfmtlqM1trZtOLX6qIiLSmzUA3syTwEHAhMAq42sxGNet2M/Cmu48FzgO+Z2blRa5VRERaUcge+gRgvbtvcPc6YAEwpVkfB3pb8KOevYAdQLqolYqISKsKCfShwKac6ZqwLdcs4HRgC7AGuM3ds81XZGYzzazazKp1ObCISHEVEuiWp6353Xa+AKwChgDjgFlm1uewhdznuHuVu1dVVlYeYakiItKaQgK9BhieMz2MYE8813RgoQfWA+8BHy9OiSIiUohCAn0ZcKqZjQy/6LwKeLZZn43AZwHMbDBwGrChmIWKiEjr2rzborunzewW4EUgCcx197VmdlM4fzZwHzDPzNYQHKK50923tWPdIiLSTEG3z3X354Hnm7XNzhnfAlxQ3NJERORI6EpREZGYUKCLiMSEAl1EJCYU6CIiMaFAFxGJCQW6iEhMKNBFRGJCgS4iEhMKdBGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhITCnQRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMaFAFxGJCQW6iEhMKNBFRGJCgS4iEhMKdBGRmCgo0M1sspm9bWbrzeyuFvqcZ2arzGytmf2huGWKiEhbUm11MLMk8BDweaAGWGZmz7r7mzl9+gH/AUx2941mdlw71SsiIi0oZA99ArDe3Te4ex2wAJjSrM81wEJ33wjg7h8Wt0wREWlLIYE+FNiUM10TtuX6GNDfzH5vZsvN7Pp8KzKzmWZWbWbVtbW1R1exiIjkVUigW542bzadAs4GLga+ANxjZh87bCH3Oe5e5e5VlZWVR1ysiIi0rM1j6AR75MNzpocBW/L02ebue4G9ZrYYGAu8U5QqRUSkTYXsoS8DTjWzkWZWDlwFPNuszy+AT5lZysx6ABOBdcUtVUREWtPmHrq7p83sFuBFIAnMdfe1ZnZTOH+2u68zsxeA14Es8GN3f6M9CxcRkUOZe/PD4R2jqqrKq6urI3luEZFSZWbL3b0q3zxdKSoiEhMKdBGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhITCnQRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMaFAFxGJCQW6iEhMKNBFRGJCgS4iEhMKdBGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURioqBAN7PJZva2ma03s7ta6TfezDJm9t+LV6KIiBSizUA3syTwEHAhMAq42sxGtdDvX4EXi12kiIi0rZA99AnAenff4O51wAJgSp5+twJPAx8WsT4RESlQIYE+FNiUM10TtjUys6HAl4DZra3IzGaaWbWZVdfW1h5prSIi0opCAt3ytHmz6R8Ad7p7prUVufscd69y96rKysoCSxQRkUKkCuhTAwzPmR4GbGnWpwpYYGYAg4CLzCzt7j8vRpEiItK2QgJ9GXCqmY0ENgNXAdfkdnD3kQ3jZjYPeE5hLiLSsdoMdHdPm9ktBGevJIG57r7WzG4K57d63FxERDpGIXvouPvzwPPN2vIGubtPO/ayRETkSOlKURGRmFCgi4jEROkFeroO3noevPmZkyIiXVvpBfrrC2DB1bDptagrERHpVEov0EdfDt36wms/jLoSEZFOpfQCvbwnnPkVWPcs7NkadTUiIp1G6QU6wISvQjYDyx+JuhIRkU6jNAN9wMlw6gVQ/UjwJamIiJRooANMmAl7P4Q3fxF1JSIinULpBvopn4EBp8Cf5kRdiYhIp1C6gZ5IwIQZUPMn2LIy6mpERCJXuoEOMO4aKOsJf/pR1JWIiESutAO9oi+MvQrWPAV7t0VdjYhIpEo70CH4cjRzEFbMj7oSEZFIlX6gH/dxGDkJlv0nZNJRVyMiEpnSD3SACV+DPTXwzq+jrkREJDLxCPSPTYa+w3V/FxHp0uIR6MkUjL8R3n8FPlwXdTUiIpGIR6ADnHk9JLvpQiMR6bLiE+g9B8InroDVC2D/rqirERHpcPEJdAiuHK3fB6sej7oSEZEOF69AHzIOhk+EZT+CbDbqakREOlS8Ah2CC412bIA/vxx1JSIiHSp+gX76pdBrsL4cFZEuJ36BniqHs6fDuy/B9j9HXY2ISIeJX6ADVE2HRDK4HYCISBcRz0DvfTyMmgIrH4ODH0VdjYhIh4hnoENwf5eDu2HNk1FXIiLSIeIb6MMnwPFj4LU54B51NSIi7S6+gW4GE78Gtevg/VejrkZEpN3FN9ABRl8O3QfAn3QXRhGJv3gHell3OOt6eOtXsGtT1NWIiLSrggLdzCab2dtmtt7M7soz/1ozez0c/mhmY4tf6lEaf2PwWD032jpERNpZm4FuZkngIeBCYBRwtZmNatbtPeDT7j4GuA/oPJdp9jsRTrsIVjwK9QeirkZEpN0Usoc+AVjv7hvcvQ5YAEzJ7eDuf3T3neHkUmBYccs8RhNmwL7tsHZh1JWIiLSbQgJ9KJB7ALombGvJjUDn+nHPkZ+GQacFP1GnUxhFJKYKCXTL05Y3Fc3sfIJAv7OF+TPNrNrMqmtrawuv8liZBXvpW1dBTXXHPa+ISAcqJNBrgOE508OALc07mdkY4MfAFHffnm9F7j7H3avcvaqysvJo6j16Y6+Gbn10F0YRia1CAn0ZcKqZjTSzcuAq4NncDmZ2IrAQuM7d3yl+mUXQrReMuwbWPgN//UvU1YiIFF2bge7uaeAW4EVgHfCku681s5vM7Kaw2z8BA4H/MLNVZtY5j2uMnwHZ+uCMFxGRmDGP6EvCqqoqr66OIPf/75fhwzfh9jWQLOv45xcROQZmttzdq/LNi/eVovlM/Br8dSus+2XUlYiIFFXXC/S/+Rz0H6EvR0UkdrpeoCeSwbH0jUtg6+tRVyMiUjRdL9ABzrwWynrAsh9FXYmISNF0zUDv3h/G/A94/Wewb0fU1YiIFEXXDHSACTMhvT/43VERkRjouoE++Aw46b8Fh12ymairERE5Zl030CG4v8uujfDub6KuRETkmHXtQP/4JdBnaHAXRhGREte1Az2ZgqrpsOF3UNs5b0EjIlKorh3oAGdNg2S5TmEUkZKnQO9VCWd8GVY9Dgf2RF2NiMhRU6ADTJwJdR/B6gVRVyIictRSURfQKQw9Oxh+ex988CoMPwdOnAjHj9EdGUWkZCjQG0x5CF75Hmx8Dd78RdCW6h4E/YkTg5AfPj64ylREpBNSoDc47nS4/MfB+J4tsOm1INw3LYVXfwAeXnxUeToMnwAnngPDJ8KAk4PfLBURiVjX+4GLo1G3FzavCMJ942tQ8yc4sDuY17MyCPbhE4OQP2EspLpFW6+IxFZrP3ChPfRClPeEkZ8KBoBsFmrfCvbiN70GG5fCW88F85LdYOhZwV788HAvvufA6GoXkS5De+jF8te/BHvuG5cGIb9lVfD7pQC9T4B+J0G/Ew8f+g6HVHmkpYtI6dAeekfoPRhO/2IwANTvhy0rg3Df9m5wz5hNS+GNp5uOxwNg0GdI/rDvdxL0HaYzbUSkIAr09lLWHU76ZDDkyqRhz+Yg4JsPH/wR1vwMPNvU3xLQu4XA71kZnHXTY4CO24uIAr3DJVPQ/6RgyCdTf2jg7/ygafz9V4N55DlMVtYzDPf+0H1AU9C3Nl7RN6hHRGJBf82dTbIs+BHr/iPyz0/XNQX+vu2wfwfs3wn7duaM7whOvWyYzt3jb66ib7Og7x8MFf3C8X6HT1f0g7KKIr9wETlWCvRSkyqHASODoRDZLBzcE4T7vp1BwOcGf+74vh2wfT3s3xWeltnKF+ap7i2Hfb4NQUWf4Hdcy3sEnyb0RbBI0SnQ4y6RCMO1Hww4guWyWTi4O9wA7AoeD+zKMx0Ouz6ArauC8fq9BdSVCoK9rHtTyJf3CEO/56Hh31p7sltw2CiRgkRZ8NjWdCKpi8EklhTokl8i0XT45Uil6w4P/4N7ggu06vcHgV+3D+r3hW37wum9cPCv8NFfmrXvo9VPC0f1+trYAJT1CDY2uRuSsu7NNir5NjQ98i+jM5WkAyjQpfhS5dDruGAoBvdwQ7Dv0PCv2weZg8FvwmbqIZtuGo5lOlPftOGp3x8cimrcCIXt2fSRvYZEWRDwybKmIVEW3Is/mQofy8MNSnmePg3L5euTCs6GwoJHa3hsaLM8bYlD+x6ybDjesJE75LmbT7dQrz4FRUKBLp2fWbD3W94j6kqapOuaAj93A5O3LWfjkKmHTF244agLh3A8W9+0MWmrT6a+6cK1zqr5BqthY5BIhhuh8DGRaDadDDcIrU2ncjY6yWDdqfLgEFyqHFIVzcbLg1N7U93C9tzxZv2T3YI6S3CDpEAXORqp8mCI8u6b7k2fLPDgbCYPH/Gm8UPasoe2HdI3d342/MRSn39jcsgGJ2cD09LGJ3c6mwmW9Uw4nmc6fTCcTgff5zTOTzf1OWQ6HSyTOVikN9eCwE80BHvOJ53cTzOtjrfS/+yp8Lc3F6nWJgp0kVJlFp4tpDOGGjVs5NIHgo1I+mCz8TD003Xh44Gc8dz54ZBNB+vM3UC2Ok4BfTy4KLAdKNBFJD4aNnJd9LRY/QSdiEhMKNBFRGKioEA3s8lm9raZrTezu/LMNzN7MJz/upmdVfxSRUSkNW0GupklgYeAC4FRwNVmNqpZtwuBU8NhJvBwkesUEZE2FLKHPgFY7+4b3L0OWABMadZnCjDfA0uBfmZ2QpFrFRGRVhQS6EOBTTnTNWHbkfbBzGaaWbWZVdfW1h5prSIi0opCAj3f5VLNb6xRSB/cfY67V7l7VWVl+5yHKSLSVRUS6DXA8JzpYcCWo+gjIiLtqM0fiTazFPAO8FlgM7AMuMbd1+b0uRi4BbgImAg86O4T2lhvLfDBUdY9CNh2lMtGoZTqLaVaobTqLaVaobTqLaVa4djqPcnd8x7iaPNKUXdPm9ktwItAEpjr7mvN7KZw/mzgeYIwXw/sA6YXsN6jPuZiZtUt/ep1Z1RK9ZZSrVBa9ZZSrVBa9ZZSrdB+9RZ06b+7P08Q2rlts3PGHSj+nWZERKRgulJURCQmSjXQ50RdwBEqpXpLqVYorXpLqVYorXpLqVZop3rb/FJURERKQ6nuoYuISDMKdBGRmCi5QG/rzo+dhZkNN7Pfmdk6M1trZrdFXVMhzCxpZivN7Lmoa2mNmfUzs6fM7K3wPf7bqGtqjZn9ffj/4A0ze8LMKqKuKZeZzTWzD83sjZy2AWb2kpm9Gz5G+Ht7TVqo9YHw/8LrZvaMmfWLsMRD5Ks3Z94/mpmb2aBiPFdJBXqBd37sLNLA/3T304FzgJs7ca25bgPWRV1EAf4deMHdPw6MpRPXbGZDgb8Dqtx9NMH1HFdFW9Vh5gGTm7XdBbzs7qcCL4fTncE8Dq/1JWC0u48huBDyGx1dVCvmcXi9mNlw4PPAxmI9UUkFOoXd+bFTcPet7r4iHP8rQeAcdsOyzsTMhgEXAz+OupbWmFkfYBLwnwDuXufuuyItqm0poHt45XUPOtmtMdx9MbCjWfMU4NFw/FHgso6sqSX5anX337h7OpxcSnD7kU6hhfcW4PvA18lz36ujVWqBXtBdHTsbMxsBnAm8FnEpbfkBwX+wbMR1tOVkoBZ4JDw89GMz6xl1US1x983A/yHYE9sK7Hb330RbVUEGu/tWCHZQgOMirqdQNwC/jrqI1pjZpcBmd19dzPWWWqAXdFfHzsTMegFPA7e7+56o62mJmV0CfOjuy6OupQAp4CzgYXc/E9hL5zkccJjw2PMUYCQwBOhpZl+Jtqp4MrNvEhzu/EnUtbTEzHoA3wT+qdjrLrVAL6m7OppZGUGY/8TdF0ZdTxvOBS41s/cJDmV9xswei7akFtUANe7e8InnKYKA76w+B7zn7rXuXg8sBD4ZcU2F+EvDD9WEjx9GXE+rzGwqcAlwrXfuC2xOIdi4rw7/3oYBK8zs+GNdcakF+jLgVDMbaWblBF8sPRtxTXmZmREc413n7v8WdT1tcfdvuPswdx9B8L7+1t075V6ku/8XsMnMTgubPgu8GWFJbdkInGNmPcL/F5+lE3+Jm+NZYGo4PhX4RYS1tMrMJgN3Ape6+76o62mNu69x9+PcfUT491YDnBX+vz4mJRXo4ZceDXd+XAc8mXsb307mXOA6gj3dVeFwUdRFxcitwE/M7HVgHPC/oy2nZeEniaeAFcAagr+7TnWpupk9ASwBTjOzGjO7Ebgf+LyZvUtwNsb9UdbYoIVaZwG9gZfCv7XZra6kA7VQb/s8V+f+ZCIiIoUqqT10ERFpmQJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhIT/x8dnKnvNZq34wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.legend(['accuracy', 'loss'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Edit the Keras MNIST code to use a simple RNN, then cheat by passing all images of a class in a sequence (careful with batch size). Reset the network between classes. RNNs will recognize that you expect a constant output per each sequence, decide which output with the first few images, then just saturate the right neurons using the recurrent connections to generate a constant output regardless of the input. You can verify this by then testing the network on a sequence of elements from a constant class, followed by one (or more) elements from another class: they will likely be misclassified. All intelligent learning picks up on shortcuts whenever available, here is a famous example (check the full paper): [husky vs. wolf](https://www.researchgate.net/figure/A-husky-on-the-left-is-confused-with-a-wolf-because-the-pixels-on-the-right_fig1_329277474). Notice that getting a \"simple\" RNN in Keras is not straightforward, and we will see LSTMs next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of this lecture + exercise you should _own_ neural networks. It does not mean that you know everything about them, but you have enough to learn from any resource you can find, and you actually know how these things work better than most people who just use Keras/Pytorch on a daily basis (unfortunately).\n",
    "- Try to get the original Keras MNIST code to work on GPUs/TPUs using Colab and get a first feel of the speed boost from specialized hardware that we will present next week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
